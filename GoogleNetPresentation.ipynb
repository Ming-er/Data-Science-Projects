{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-26T12:12:53.279550Z",
     "start_time": "2019-10-26T12:12:53.275526Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Latex\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 神经网络基础 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "人工神经网络，是20世纪80 年代以来人工智能领域兴起的研究热点。它从信息处理角度对人脑神经元网络进行抽象， 建立某种简单模型，按不同的连接方式组成不同的网络。在工程与学术界也常直接简称为神经网络或类神经网络。神经网络是一种运算模型，由大量的节点（或称神经元）之间相互联接构成。每个节点代表一种特定的输出函数，称为激励函数。每两个节点间的连接都代表一个对于通过该连接信号的加权值，称之为权重，这相当于人工神经网络的记忆。网络的输出则依网络的连接方式，权重值和激励函数的不同而不同。而网络自身通常都是对自然界某种算法或者函数的逼近，也可能是对一种逻辑策略的表达。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![NN.jpg](https://i.loli.net/2019/10/21/DxQryAwusmMcR5P.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上是百度百科对神经网络的定义，看起来可能很玄乎，但是说白了神经网络其实就是一个多元函数，它给出了从输入特征到预测结果的一个映射，而映射的法则则是从海量的数据当中学习出来的，其中蕴藏着一定的统计与优化的知识。在百度百科的定义中，有几个概念是值得深究的，一是神经元(也就是结点)，它代表着一种特定的输出函数——激励函数，激励函数对应着数据的非线性变换；二是结点间的连接，它是信息从一层神经元传到另一层神经元的传播，这种传播对应着数据的线性变换，这些概念在下面将会提到"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 正向传播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "神经网络的正向传播过程即是给定输入计算输出的过程，我们来看一个浅层神经网络的例子，来简单地了解一下这种传播是如何进行的\n",
    "![nn1.png](https://i.loli.net/2019/10/23/rD9Mkn3i1uCaZjo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于一个简单的神经网络，其第一层称为输入层，即输入的特征x，它也可以被看成一个n维的向量(n是特征的数量)，比如我们要识别一张图片是猫还是狗，那么输入的就有可能是一个784维的向量，每一个元素代表了一个像素(在这里我们假定图片的像素为$28\\times28$)，中间用于传递计算的神经元被称为隐藏层，这得名于在训练过程中它们的更新信息很大程度上是个黑匣子，最后一层对应着输出层，还是以猫狗识别为例，那么输出层的两个神经元就分别代表着图片是猫还是狗的概率，哪一个大，我们就预测整张图片为哪一个，现在我们做一些符号约定\n",
    "规定$x_i$代表第二个特征，$\\omega_j^{[i]}$代表第i层的第j个权重，$b_j^{[i]}$代表第i层第j个神经元所对应的偏移，对于偏移，你可以把它理解为截距或是一个标量，$z_j^{[i]}$代表第i个隐层层第j个神经元前端输入的计算结果$a_j^{[i]}$代表第i个隐层层第j个神经元后端输出的计算结果，$y_i$代表最后的第i个神经元的输出，$\\sigma(z)$代表对$z$所作的激活函数，为了让大家有一个稍微具象一些的感受，这里给出一个常用激活函数的形式，这个函数被称为sigmoid函数$$\\sigma(z)=\\frac{1}{1+e^{-z}}$$，下面我们先看一个局部传播的简单例子![propagation.png](https://i.loli.net/2019/10/23/LaOjvDg8lJ5MdH7.png)这是三个输入神经元向一个隐藏层神经元传播的例子，下方给出了计算的相应公式，即$$z_{1}^{[1]}=w_{1}^{[1] T} x+b_{1}^{[1]}$$$$a_{1}^{[1]}=\\sigma\\left(z_{1}^{[1]}\\right)$$(如果不理解可以简单说明一下...)，那么对于一个更复杂的情形，将三个输入神经元传播到隐藏层的四个神经元，结果只是对上式的复制![propagation2.png](https://i.loli.net/2019/10/23/3Qg6jOT2LPzv4DH.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-23T13:49:40.795379Z",
     "start_time": "2019-10-23T13:49:40.785406Z"
    }
   },
   "source": [
    "$$\n",
    "\\begin{array}{l}{z_{1}^{[1]}=w_{1}^{[1] T} x+b_{1}^{[1]}, a_{1}^{[1]}=\\sigma\\left(z_{1}^{[1]}\\right)} \\\\ {z_{2}^{[1]}=w_{2}^{[1] T} x+b_{2}^{[1]}, a_{2}^{[1]}=\\sigma\\left(z_{2}^{[1]}\\right)} \\\\ {z_{3}^{[1]}=w_{3}^{[1] T} x+b_{3}^{[1]}, a_{3}^{[1]}=\\sigma\\left(z_{3}^{[1]}\\right)} \\\\ {z_{4}^{[1]}=w_{4}^{[1] T} x+b_{4}^{[1]}, a_{4}^{[1]}=\\sigma\\left(z_{4}^{[1]}\\right)}\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果你执行神经网络的程序，用for循环来做这些看起来真的很低效,所以接下来我们要做的就是把这四个等式向量化,向量化的过程是将神经网络中的一层神经元参数纵向堆积起来，例如隐藏层中的$\\omega$纵向堆积起来变成一个的矩阵，用符号$W^{[1]}$表示,另一个看待这个的方法是我们有四个逻辑回归单元，且每一个逻辑回归单元都有相对应的参数——向量$\\omega$，把这四个向量堆积在一起，你会得出这4×3的矩阵。\n",
    "那么就有，$$z^{[n]}=w^{[n]} x+b^{[n]}$$，$$a^{[n]}=\\sigma\\left(z^{[n]}\\right)$$\n",
    "更具体地，$$\\left[\\begin{array}{c}{z_{1}^{[1]}}  \\\\ {z_{2}^{[1]}} \\\\ {z_{3}^{[1]}} \\\\ {z_{4}^{[1]}}\\end{array}\\right]=\\left[\\begin{array}{c}{\\ldots W_{1}^{[1] T} \\ldots} \\\\ {\\ldots W_{2}^{[1] T} \\ldots} \\\\ {\\cdots W_{3}^{[1] T} \\ldots} \\\\ {\\ldots W_{4}^{[1] T} \\ldots}\\end{array}\\right] *\\left[\\begin{array}{c}{x_{1}} \\\\ {{x}_{2}} \\\\ {x_{3}}\\end{array}\\right]+\\left[\\begin{array}{l}{b_{1}^{[1]}} \\\\ {b_{2}^{[1]}} \\\\ {b_{3}^{[1]}} \\\\ {b_{4}^{[1]}}\\end{array}\\right]$$\n",
    "值得注意的是，神经网络的训练通常需要有很多样本，而我们上面只是简单地给出了样本容量为1时的情形，那么我们只需要在另一个维度上对x进行叠加即可，这时候的x就变成了一个二维矩阵，如以下是有$m$个样本的情形，$$x=\\left[\\begin{array}{cccc}{\\vdots} & {\\vdots} & {\\vdots} & {\\vdots} \\\\ {x^{(1)}} & {x^{(2)}} & {\\dots} & {x^{(m)}} \\\\ {\\vdots} & {\\vdots} & {\\vdots} & {\\vdots}\\end{array}\\right]$$\n",
    "那么这时$a$与$z$也不再只是简单的列向量，而是包含了m个样本信息的二维矩阵了，有\n",
    "$$Z^{[1]}=\\left[\\begin{array}{cccc}{\\vdots} & {\\vdots} & {\\vdots} & {\\vdots} \\\\ {z^{[1](1)}} & {z^{[1](2)}} & {\\dots} & {z^{[1](m)}} \\\\ {\\vdots} & {\\vdots} & {\\vdots} & {\\vdots}\\end{array}\\right]$$\n",
    "$$A^{[1]}=\\left[\\begin{array}{cccc}{\\vdots} & {\\vdots} & {\\vdots} & {\\vdots} \\\\ {a^{[1]}(1)} & {a^{[1](2)}} & {\\cdots} & {a^{[1](m)}} \\\\ {\\vdots} & {\\vdots} & {\\vdots} & {\\vdots}\\end{array}\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 激活函数 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为什么我们需要在每个神经元节点使用激活函数激活呢？这点是十分显然的，因为如果我们只使用$z=w^{T} x+b$的形式来传递输入数据，那么神经网络将只能表征线性变换，即我们人为地限定了输入特征与目标输出之间一定是线性的关系，那么这样不管多深的神经网络在本质上都只相当于有一层，举一个很简单的例子对于$y_1=a_1*x+b_1$，如果我们再向后传播一次$y_2=a_2*y_1+b_2$，那么展开这个式子$y_2=a_1*a_2*x+a_2*b_1+b_2$这个式子本质上来看还是$y=a*x+b$的形式，也就是说$a_1*a_2$与a是无异的，因为我们都可以通过数据学到它们。可想而知，加入激活函数，模型将具有表征非线性的能力，因为这时候层与层之间的传播不再只是简单的线性传递，而多了一些曲线形式的变换。  \n",
    "使用一个神经网络时，需要决定使用哪种激活函数用隐藏层上，哪种用在输出节点上。在之前的例子中我们只使用了sigmoid函数，但是，有时其他的激活函数效果会更好。几乎在所有情况下，tanh函数($\\tanh (z)=\\frac{e^{z}-e^{-z}}{e^{z}+e^{-z}}$)是总体上都优于sigmoid函数的激活函数(事实上，tanh函数是将sigmoid函数在纵轴上做了拉伸与平移),因为函数值域在-1和+1的激活函数，其均值是更接近零均值的。在训练一个算法模型时，如果使用tanh函数代替sigmoid函数中心化数据，使得数据的平均值更接近0而不是0.5，这将更有利于之后的学习。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-26T12:44:51.191027Z",
     "start_time": "2019-10-26T12:44:51.033176Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x20d678f7208>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "def tanh(x):\n",
    "    return (np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "x = np.linspace(-5,5,100)\n",
    "tanh_y = tanh(x)\n",
    "sigmoid_y = sigmoid(x)\n",
    "plt.plot(x,tanh_y)\n",
    "plt.plot(x,sigmoid_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然而现在学术界最常使用的函数是ReLu(线性修正单元)函数，包括在下面的GoogleNet中也是使用的ReLu函数，这是基于tanh和sigmoid函数都具有的两个缺点：1.幂运算开销过大 2.当x值远离0时，函数的梯度快速趋向于零。第一个理由很容易理解，而第二个理由则与下面要提到的反向传播有关$$ReLu(x)=\\max (0, x)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-26T12:12:54.544515Z",
     "start_time": "2019-10-26T12:12:54.385179Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x20d67891c50>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def ReLu(x):\n",
    "    return 0 if x <= 0 else x\n",
    "x = np.linspace(-5,5,100)\n",
    "ReLu_y = x.copy()\n",
    "ReLu_y[ReLu_y < 0] = 0\n",
    "plt.plot(x,ReLu_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 反向传播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "神经网络最难理解的一部分莫过于反向传播，所以在这里我们不详细介绍它，而是给出一个直观的印象。我们都知道神经网络的目的是要从训练集中学习参数，那么怎样的参数学习是成功的呢？我们将每次神经网络预测的结果与真实值做一个差值，这个差值被称为损失项，我们的目的就是要最小化这个损失项。当提到最小化一个函数时，第一直觉肯定是求导找极值点，但是对于神经网络来说这并不奏效，因为神经网络的参数实在太庞大了，并且损失函数可能并不是凸函数，这时候我们便需要一个数值上的极小化方式——梯度下降，根据高数知识，我们知道梯度是方向导数的最大值，梯度的方向即是函数增长最快的方向(反方向即是下降最快的方向)，所以如果我们如果每次沿梯度方向走一小段距离，那么我们就可以保证函数值一定是下降的，基于这个思想，我们可以每次让所有的参数都在梯度方向上减小一点，一直迭代下去，直到收敛即可。而反向传播则是计算梯度的一种方法，它通过链式求导法则实现。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 卷积神经网络 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   卷积神经网络是含有卷积层的神经网络,并被逐渐应用在诸如自然语言处理、推荐系统和语音识别等领域。卷积神经网络是近年来深度学习能在计算机视觉领域取得突破性成果的基石，其中有两个尤为重要的“层”的概念——卷积层(Convolution)与池化层(Pooling)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-19T08:54:16.429359Z",
     "start_time": "2019-10-19T08:54:16.360350Z"
    }
   },
   "source": [
    "![CNN.png](https://i.loli.net/2019/10/21/uavkPhSzNOVpLTY.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 卷积层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 互相关运算(卷积运算)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-19T08:57:50.154383Z",
     "start_time": "2019-10-19T08:57:50.148399Z"
    }
   },
   "source": [
    "在二维卷积层中，一个二维输入数组和一个二维核数组（卷积核）通过互相关运算，并在此基础上加上一个标量偏差，最终得到一个二维输出数组。在训练模型的时候，通常我们先对卷积核随机初始化，然后不断迭代卷积核与偏差。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![convolution.png](https://i.loli.net/2019/10/21/eAD4lZo2wmWpyOf.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如上图所示，输入是一个高和宽均为3的二维数组，而核数组的高和宽分别为2(该数组在卷积计算中又称卷积核或过滤器)，图中的阴影部分为第一个输出元素19及其计算所使用的输入和核数组元素对应位置的乘积：0×0+1×1+3×2+4×3=19(也即二者哈达玛积之和)，以此类推，将卷积核按照从左到右、从上到下的顺序“映射”在输入数组上并与其做互相关运算，即可得到最终的输出数组："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$output_{11}=0\\times0+1\\times1+3\\times2+4\\times3=19$$\n",
    "$$output_{12}=1\\times0+2\\times1+4\\times2+5\\times3=25$$\n",
    "$$output_{21}=3\\times0+4\\times1+6\\times2+7\\times3=37$$\n",
    "$$output_{22}=4\\times0+5\\times1+7\\times2+8\\times3=43$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不难假设输入形状是(m,n)，卷积核形状是(h,w)，则最终输出数组的形状是$(m-h+1)\\times(n-w+1)$，下面是卷积层的一个简单实现与测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-26T12:12:54.557471Z",
     "start_time": "2019-10-26T12:12:54.548045Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#定义输入数组X与卷积核K的卷积运算\n",
    "def corr(X, K):\n",
    "    h, w = K.shape\n",
    "    Y = np.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            Y[i, j] = (X[i: i + h, j: j + w] * K).sum()\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-26T12:12:54.579373Z",
     "start_time": "2019-10-26T12:12:54.560462Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[19., 25.],\n",
       "       [37., 43.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#测试卷积层\n",
    "X = np.array([[0, 1, 2], [3, 4, 5], [6, 7, 8]])\n",
    "K = np.array([[0, 1], [2, 3]])\n",
    "corr(X, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 为什么要卷积？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "卷积运算到底有什么用呢？下面我们来看一个关于边缘检测的demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![edge.png](https://i.loli.net/2019/10/21/IvGF4UiEdp8bcVa.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-26T12:12:54.595644Z",
     "start_time": "2019-10-26T12:12:54.583363Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.]]), array([[ 1,  0, -1],\n",
       "        [ 1,  0, -1],\n",
       "        [ 1,  0, -1]]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.ones((6, 8))\n",
    "X[:, 2:6] = 0\n",
    "K = np.array([[1,0,-1],[1,0,-1],[1,0,-1]])\n",
    "X,K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-26T12:12:54.610323Z",
     "start_time": "2019-10-26T12:12:54.600418Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.,  3.,  0.,  0., -3., -3.],\n",
       "       [ 3.,  3.,  0.,  0., -3., -3.],\n",
       "       [ 3.,  3.,  0.,  0., -3., -3.],\n",
       "       [ 3.,  3.,  0.,  0., -3., -3.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = corr(X, K)\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-19T14:56:15.403598Z",
     "start_time": "2019-10-19T14:56:15.399609Z"
    }
   },
   "source": [
    "在这里，我们假设X是一张像素为8x8的二值灰度图像(1代表黑色，0代表白色)，其图像的左右两侧是黑色、中间是白色，那我们可以直观想象一下图像应该会有一个较为明显的黑色边框，我们观察X与K做卷积运算后得到的数组，发现在这个数组的两侧也出现了非零的部分，这就说明原来图像的信息在经过卷积处理后得到了提取，不失一般性得，如果图中的边框是横向的，我们也同样可以采用相应的卷积核提取其横向的边框，而事实上，随着计算机视觉近期的蓬勃发展，边缘检测也得到了飞速的发展，目前已经有许多成熟的算法用来解决不同的边缘检测问题，当然在这里我们可以得出一个关于卷积层的结论，卷积层可以用于提取空间中的模式特征。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 卷积的其他重要概念 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 多输入通道与多输出通道情形下的卷积 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前面两节里我们用到的输入和输出都是二维数组，但真实数据的维度经常更高。例如，彩色图像在高和宽2个维度外还有RGB（红、绿、蓝）3个颜色通道。假设彩色图像的高和宽分别是$h$和$w$（像素），那么它可以表示为一个$3\\times h\\times w$的多维数组。我们将大小为3的这一维称为通道维。  \n",
    "对于多输入通道的情形，有一个规定是卷积核的输入通道数必须与数据的输入通道数是相同的，来看一个例子，![multi1.png](https://i.loli.net/2019/10/25/QWridmYcaLZ1JPf.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这是一个二输入通道的数据，你可以把通道维想象成$z$轴，那么就相当于这个二维数组的\"厚度\"是2，那么我们以$z$为轴，将$z$轴上的每一个数组都与一个卷积核做卷积，这样就保证了卷积核的\"厚度\"(个数)与输入通道的\"厚度\"(个数)相同了。依次进行卷积运算后，将各个结果在$z$轴上累加起来即是最终结果。  \n",
    "当输入通道有多个时，因为我们对各个通道的结果做了累加，所以不论输入通道数是多少，输出通道数总是为1。设卷积核输入通道数和输出通道数分别为$c_i$和$c_o$，高和宽分别为$k_h$和$k_w$。如果希望得到含多个通道的输出，我们可以为每个输出通道分别创建形状为$c_i\\times k_h\\times k_w$的核数组。将它们在输出通道维上连结，卷积核的形状即$c_o\\times c_i\\times k_h\\times k_w$。在做互相关运算时，每个输出通道上的结果由卷积核在该输出通道上的核数组与整个输入数组计算而来。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  填充"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上面的示例中我们可以看到，如果我们使用一个2x2的卷积核去卷积一个3x3的输入数组，那么最终将会得到一个2x2的输出数组，并且我们推导得出了输出数组的维度为$(m-h+1)\\times(n-w+1)$，不难发现如果我们使用的卷积核的大小大于1，那么输出数组的大小将会一直减小，想象一下我们现在有一张3x64x64的图像，每经过一次卷积它的大小便减小一点，那么可以想象，当网络深度很深的时候，图像将变得很小，其中很多的有效信息将被遗失。还有一个问题是，当我们使用卷积核对输入数组进行卷积的时候，处于边缘的像素点将只被计算一次，但如果是在中间的像素点，就会有许多区域与之重叠也就被计算多次，那么图像边缘的信息在输出中被采用的很少，换句话说，图像边缘的信息被丢失了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-20T05:46:15.981078Z",
     "start_time": "2019-10-20T05:46:15.976138Z"
    }
   },
   "source": [
    "为了解决上面提到的这两个问题，我们可以对图像采取填充(padding)的方式来规避问题，如下，我们对一个6x6的输入数组的上下左右各填充上行列，得到一个8x8的数组(通常填充的元素是0，当然也有别的选择)，这时再对它进行卷积，就可以得到一个6x6的输出数组了，对于我们开始时得到的公式：输出数组大小$(m-h+1)\\times(n-w+1)$，如果我们现在考虑填充大小为p，那么它将变为:输出数组大小$(m-h+2p+1)\\times(n-w+2p+1)$。例如对于一个5x5的卷积核，我们只需要对其填充两行两列就可以使输入输出的大小保持一致，而3x3、7x7的卷积核可以以此类推。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![padding.png](https://i.loli.net/2019/10/21/RxflZgrLKbkOSHD.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在GoogleNet及其他有名的网络结构中，通常使用的卷积核都是奇数维度大小的如3x3、5x5等，在有了填充的概念后，我们就可以对这种现象做出解释。如果我们使用了偶数大小的卷积核，那么根据输出数组大小$(m-h+2p+1)\\times(n-w+2p+1)$，如果我们想保证输出数组的大小保持不变，那么我们就不能在图像的两侧都添加相同数目的行列，在这种情形下，这种填充就成为了不对称的填充，它将影响我们对图像边缘信息的采集。  \n",
    "使用奇数维度大小的卷积核在实际操作上还有一个好处是，奇数大小的卷积核存在一个中心的元素，这便于我们来描述卷积核当前所处的位置。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 步长 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![stride.png](https://i.loli.net/2019/10/21/HZtWDyRBKi4znJd.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "步长的大小反映了卷积核每次在输入数组上移动的位置信息，如上图是横向步长为2、纵向步长为3时的情形，即卷积核每次完成卷积后沿横向移动两个单元，而需要换行时，则沿纵向移动三个单元，如果考虑再将步长因素(stride,简记为s)纳入考虑，那么输出数组大小的公式变为我们需要的最终情形——$$(\\lfloor\\frac{(m+2p-h)}{s}\\rfloor+1)\\times(\\lfloor\\frac{(n+2p-w)}{s}\\rfloor+1)$$  \n",
    "但是在设置步长时有一个需要注意的地方是，如果卷积核移动到输入数组的右侧时，右侧所剩的宽度不足卷积核的宽度，那么这次卷积运算就不会发生，这就是公式中下取整符号的来历，当然这就意味着我们可能会丢失右侧边缘的某些信息，因而我们在步长选择时，通常会使得公式中的除法能够得到整除。还有一个十分显然但是值得注意的问题是，步长的选择最好不要超过卷积核的大小，这样也会造成某些行列特征信息的丢失。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 一种“特殊”的卷积——1*1卷积"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为使用了最小的窗口，这种卷积失去了卷积层可以识别高和宽维度上相邻元素构成的模式的功能，并且这种卷积并不会改变图像的高与宽。输出中的每个元素来自输入中在高和宽上相同位置的元素在不同通道之间的按权重相加，其本质上是输入数组中元素的线性重组。因此，卷积层的作用（在特征组合上）几乎与全连接层等价。但是，通过设置1*1卷积的输出通道数，可以对数据的输出通道数进行调整，而又不丢失特定位置的数据元素的相关信息，经常可以通过调整网络层之间的通道数来控制模型复杂度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![11convolution.png](https://i.loli.net/2019/10/21/ivwpKrfmdkzlSEs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上图即是用一个输出通道数为2的1*1卷积核（卷积层大小为(3,2,1,1)），将输入通道数为3的数据调整为输出通道为2的数据，从而有效地控制了模型复杂度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 池化运算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同卷积层类似，池化层每次对输入数据的一个固定形状的窗口（池化窗口）中的元素计算输出。不同于卷积层里计算输入和核的互相关性，池化层直接计算池化窗口内元素的最大值或者平均值，也称最大池化或平均池化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![pool.png](https://i.loli.net/2019/10/21/geOGKd3yhtlQ4o2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上图即是对输入数组做2*2最大池化（对窗口中元素进行max运算）得到的输出数组，而如果是平均池化层，则是对窗口内的所有元素取均值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 为什么要池化？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上面可以看出，池化就是把那一小片的信息综合一些，整合成一个信息。  \n",
    "这样做有两个明显的优势，一是如果有数据在这一小片范围内变动，一般不太会影响池化的结果，换句话说，就是对降低了模型在这个位置上的敏感性；二是池化层对输入的特征图进行了压缩，简化了神经网络的计算，但却保留了其主要特征。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 写在最后的总结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-19T14:17:44.316145Z",
     "start_time": "2019-10-19T14:17:44.312161Z"
    }
   },
   "source": [
    "①模式特征出现在检测图像的较小区域。  \n",
    "如我们要探测鸟喙，而鸟喙并不会出现在一整张图片中而是出现在某一个特定的区域。  \n",
    "②相似的模式特征可以在图像的不同区域被探测到。  \n",
    "如我们对于不同的图片，鸟喙可能出现在图片的不同区域，但都不会是整张图片，而卷积核在图像上左右平移则可以提取这些特征  \n",
    "③在图像中抽取特定行列的像素值，并不会在宏观上对图像造成太大的改变与影响。  \n",
    "如将一张待辨识的鸟的图像抽取其中部分像素后所形成的新图像看起来仍然是一只鸟，只不过稍显模糊(观其大略)。  \n",
    "而卷积神经网络的主要设计也与这些适用范围有着天然的联系。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![summary.png](https://i.loli.net/2019/10/21/6zqKJZitsGh9evE.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GoogleNet的核心——Inception结构(v1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Inception.png](https://i.loli.net/2019/10/21/EKNMOdwcaWzF1t9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特征与优势"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i 采用不同大小的卷积核意味着不同大小的感受野，最后拼接意味着不同尺度特征的融合，同时这种不同尺度空间特征的抽取是并行运算的。  \n",
    "ii 之所以卷积核大小采用(1,1)、(3,3)和(5,5)，主要是为了方便对齐，设定卷积步长等于1之后，只要分别设定填充分别等于0、1、2，采用卷积计算可以得到相同维度的特征，然后这些特征直接拼接在一起(在输出通道这一维度上拼接，输出通道数即感受野的厚度)。  \n",
    "iii 由于(3,3)和(5,5)的卷积核仍然会带来巨大的计算成本,这是因为所需卷积核元素的个数与卷积核大小的平方成正比、也与输入通道数和输出通道数成正比$$num \\propto kernelsize^2\\times inputchannel \\times outputchannel$$所以在(3,3)与(5,5)的卷积核之前、最大池化层之后分别加入了(1,1)的卷积核，起到了降低通道数个数的作用，显著地降低了需要学习的参数。  \n",
    "iv 网络越到后面特征越抽象，且每个特征涉及的感受野也更大，随着层数的增加，3x3和5x5卷积的比例也要相应增加，甚至采用更大的卷积层（这是后话，如果没记错的话改进的GoogleNet结构就新加了许多7x7的卷积层）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一点额外的头脑风暴---What does CNN learn?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于上面的第四点是我在读别人研究论文时一个很神奇的结论，也是我在学习卷积神经网络时颇惊艳我的一个想法，因为它在某种程度上打破了神经网络的黑匣子。现在考虑一个情景，我们已经训练好了一个神经网络，知道了它的某个卷积层的参数，现在我们想要知道这个卷积层从图像当中学到了什么，首先我们定义一个指标叫做ActiveDegree，并有$$ActiveDegree=\\sum_{i=1}^m \\sum_{j=1}^n filter_{i,j}$$它代表了一个卷积核被激活的程度即卷积核内所有元素的和，注意之前我们是对一个确定的输入X(像素矩阵)，学习到最佳的filter内的参数，现在问题反过来了，我们已知filter的值，想要学到一个输入数组X，使得$$\n",
    "X^{*}=\\arg\\max _{X} ActiveDegree$$这时我们就可以使用梯度上升算法(Gradient ascent)来更新X使其最大化,在研究中，他们给出了能maximize实验中八个卷积核的那些个X，结果如下，![texture.png](https://i.loli.net/2019/10/21/lLSMABQdGioNyUE.png) \n",
    "如上面第一行的第三张图，它说明这个卷积核在学习一种能够探测斜条纹的\"本领\"，而第二行的第一个则说明卷积核在学习一种能够探测水平条纹的\"本领\"，这些特征都是十分局部的特征，他们紧接着又如法炮制地研究了一个相对靠后的全连接层，得到的结果如下![digit.png](https://i.loli.net/2019/10/21/k3oN2ZUrpfYd9Rm.png)如上面的第二行第一张图，它说明这个卷积核学到了一个本领，如果输入的图像中中间有一个小小的圆形的眼的结构，那么它就会被激活，不难发现，这时候检测到的特征更像是从全局上来把握，那么我们就有了一个更大胆的想法，如果我们将最后的输出层取出，用相同的方法去找到那个argmax的X时，那么我们得到的图像应该就趋近于数字，然而事与愿违，我们最终得到的结果是这样的，![digit2.png](https://i.loli.net/2019/10/23/pnkiM1dObJ2oG65.png)看起来很像是电视没有信号时雪花漫天飞舞的场景，但是，当真的把这几幅图片丢到CNN里时，结果却是第一张图片输出的分类结果正是0，第二张正是1...也就是说CNN理解图片并不是像我们人类一样理解图像，但是对于这些图片，直觉告诉我们这些肯定不是手写数字，因为手写数字图片里数字(白色)应该只出现在一个区域内，且占较少一部分，也就是说我们希望CNN学到的图片，白色尽可能少一些，黑色尽可能多一些，于是我们对所学到的输入X做一些修正$$x^{*}=\\arg \\max _{x}\\left(ActiveDegree-\\sum_{i, j}\\left|x_{i j}\\right|\\right)$$上式的意思是我们希望让学到的ActiveDegree尽可能大一些，同时让$\\sum_{i, j}\\left|x_{i j}\\right|$尽可能小一些($\\left|x_{i j}\\right|$代表的是x内的像素之和，我们对其做极小化处理，使白点尽可能少一些)，最后我们得到了如下的结果，从下面这张图里我们可以看出相应的数字已经初具轮廓了![digit3.png](https://i.loli.net/2019/10/23/eu8n5dlp9v7jDFa.png)如果大家对这个想法很感兴趣，油管上也有一个有趣的短视频《Deep neural networks are easily fooled》，附上视频网址https://www.youtube.com/watch?v=M2IebCN9Ht4 (如果需要翻墙也可以找我哇)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 简单实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-26T12:12:58.462060Z",
     "start_time": "2019-10-26T12:12:54.615278Z"
    }
   },
   "outputs": [],
   "source": [
    "import d2lzh as d2l\n",
    "from mxnet import gluon, init, nd\n",
    "from mxnet.gluon import nn\n",
    "\n",
    "class Inception(nn.Block):\n",
    "    # c1 - c4为每条线路里的层的输出通道数\n",
    "    def __init__(self, c1, c2, c3, c4, **kwargs):\n",
    "        super(Inception, self).__init__(**kwargs)\n",
    "        # 线路1，单1 x 1卷积层\n",
    "        self.p1_1 = nn.Conv2D(c1, kernel_size=1, activation='relu')\n",
    "        # 线路2，1 x 1卷积层后接3 x 3卷积层\n",
    "        self.p2_1 = nn.Conv2D(c2[0], kernel_size=1, activation='relu')\n",
    "        self.p2_2 = nn.Conv2D(c2[1], kernel_size=3, padding=1,\n",
    "                              activation='relu')\n",
    "        # 线路3，1 x 1卷积层后接5 x 5卷积层\n",
    "        self.p3_1 = nn.Conv2D(c3[0], kernel_size=1, activation='relu')\n",
    "        self.p3_2 = nn.Conv2D(c3[1], kernel_size=5, padding=2,\n",
    "                              activation='relu')\n",
    "        # 线路4，3 x 3最大池化层后接1 x 1卷积层\n",
    "        self.p4_1 = nn.MaxPool2D(pool_size=3, strides=1, padding=1)\n",
    "        self.p4_2 = nn.Conv2D(c4, kernel_size=1, activation='relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        p1 = self.p1_1(x)\n",
    "        p2 = self.p2_2(self.p2_1(x))\n",
    "        p3 = self.p3_2(self.p3_1(x))\n",
    "        p4 = self.p4_2(self.p4_1(x))\n",
    "        return nd.concat(p1, p2, p3, p4, dim=1)\n",
    "        # 在通道维上连结输出"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GoogNet架构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Googlenet.jpg](https://i.loli.net/2019/10/21/VgMOADPt7mJRCiL.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GoogLeNet在主体卷积部分中使用了5个模块，每个模块之间使用了大小为3x3，步长为2的最大池化层来降低输出高度。  \n",
    "第一个模块使用了一个64通道的7x7卷积层，第二个模块首先使用了64通道的1x1卷积层，然后是将通道增大3倍的3x3卷积层，第三个模块使用了2个串联的Inception块，第四个模块使用了5个串联的Inception块，第五个模块使用了2个串联的模块（如上文所述，随着层数的增加，3x3和5x5卷积的比例也相应增加了）。在这些Inception块之间的不同深度增加了两个loss来避免梯度回传消失的过程。  \n",
    "在第五个模块后紧跟输出层，其没有使用全连接层进行过渡而是使用了全局平均池化层，最后连接用于分类的神经元。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-26T12:12:58.757002Z",
     "start_time": "2019-10-26T12:12:58.466685Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequential0 output shape:\t (1, 64, 24, 24)\n",
      "sequential1 output shape:\t (1, 192, 12, 12)\n",
      "sequential2 output shape:\t (1, 480, 6, 6)\n",
      "sequential3 output shape:\t (1, 832, 3, 3)\n",
      "sequential4 output shape:\t (1, 1024, 1, 1)\n",
      "dense0 output shape:\t (1, 10)\n"
     ]
    }
   ],
   "source": [
    "b1 = nn.Sequential()\n",
    "b1.add(nn.Conv2D(64, kernel_size=7, strides=2, padding=3, activation='relu'),\n",
    "       nn.MaxPool2D(pool_size=3, strides=2, padding=1))\n",
    "b2 = nn.Sequential()\n",
    "b2.add(nn.Conv2D(64, kernel_size=1, activation='relu'),\n",
    "       nn.Conv2D(192, kernel_size=3, padding=1, activation='relu'),\n",
    "       nn.MaxPool2D(pool_size=3, strides=2, padding=1))\n",
    "b3 = nn.Sequential()\n",
    "b3.add(Inception(64, (96, 128), (16, 32), 32),\n",
    "       Inception(128, (128, 192), (32, 96), 64),\n",
    "       nn.MaxPool2D(pool_size=3, strides=2, padding=1))\n",
    "b4 = nn.Sequential()\n",
    "b4.add(Inception(192, (96, 208), (16, 48), 64),\n",
    "       Inception(160, (112, 224), (24, 64), 64),\n",
    "       Inception(128, (128, 256), (24, 64), 64),\n",
    "       Inception(112, (144, 288), (32, 64), 64),\n",
    "       Inception(256, (160, 320), (32, 128), 128),\n",
    "       nn.MaxPool2D(pool_size=3, strides=2, padding=1))\n",
    "b5 = nn.Sequential()\n",
    "b5.add(Inception(256, (160, 320), (32, 128), 128),\n",
    "       Inception(384, (192, 384), (48, 128), 128),\n",
    "       nn.GlobalAvgPool2D())\n",
    "\n",
    "net = nn.Sequential()\n",
    "net.add(b1, b2, b3, b4, b5, nn.Dense(10))\n",
    "X = nd.random.uniform(shape=(1, 1, 96, 96))\n",
    "net.initialize()\n",
    "for layer in net:\n",
    "    X = layer(X)\n",
    "    print(layer.name, 'output shape:\\t', X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考资料与拓展 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 《动手学深度学习》  \n",
    "2. bilibili 搜索\"吴恩达深度学习\"或\"李宏毅深度学习\"，有条件可以到coursera完成相应课程\n",
    "3. CVPR、ICCV、NIPS等论文(谷歌搜索即可)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
